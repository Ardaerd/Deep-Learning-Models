{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yfinance as yf\n",
    "import talib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.trial import TrialState\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau \n",
    "import shap\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"gpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA version:', torch.version.cuda)\n",
    "print('cuDNN version:', torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2018-01-01'\n",
    "end_date = '2024-01-24'\n",
    "\n",
    "stock_data = yf.download(\"AAPL\", start=start_date, end=end_date)[[\"Adj Close\", \"High\", \"Low\", \"Volume\"]]\n",
    "\n",
    "stock_data = stock_data.reset_index()\n",
    "\n",
    "stock_data = stock_data[['Date', 'Adj Close', \"High\", \"Low\", \"Volume\"]]\n",
    "\n",
    "stock_data = stock_data.sort_values(by=\"Date\")\n",
    "stock_data.head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = int((len(stock_data)-time_step)*0.8+time_step+time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = stock_data[\"Date\"].iloc[time_step:].dt.strftime('%Y-%m-%d')\n",
    "date_test = stock_data[\"Date\"].iloc[test_index:].reset_index()\n",
    "date_test.drop(columns=[\"index\"], inplace=True)\n",
    "date_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(data, timeperiod=time_step):\n",
    "\n",
    "    # MACD\n",
    "    macd, macdsignal, macdhist = talib.MACD(data[\"Adj Close\"], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "    rsi = talib.RSI(data[\"Adj Close\"], timeperiod=14)\n",
    "\n",
    "    # CMO\n",
    "    cmo = talib.CMO(data[\"Adj Close\"], timeperiod=timeperiod)\n",
    "\n",
    "    # MOM\n",
    "    mom = talib.MOM(data[\"Adj Close\"], timeperiod=timeperiod)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    upperband, middleband, lowerband = talib.BBANDS(data[\"Adj Close\"], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "\n",
    "    # SMA\n",
    "    sma_s = talib.SMA(data[\"Adj Close\"], timeperiod=20)\n",
    "    sma_l = talib.SMA(data[\"Adj Close\"], timeperiod=50)\n",
    "\n",
    "    # Calculate Exponential Moving Average (EMA)\n",
    "    ema = talib.EMA(data[\"Adj Close\"], timeperiod=timeperiod)\n",
    "\n",
    "    # Calculate Stochastic Oscillator\n",
    "    slowk, slowd = talib.STOCH(data['High'], data['Low'], data['Adj Close'], fastk_period=14, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "\n",
    "    # Calculate Average True Range (ATR)\n",
    "    atr = talib.ATR(data['High'], data['Low'], data['Adj Close'], timeperiod=14)\n",
    "\n",
    "    # Calculate On-Balance Volume (OBV)\n",
    "    obv = talib.OBV(data['Adj Close'], data['Volume'])\n",
    "\n",
    "    # Combine all indicators into a DataFrame\n",
    "    indicators = pd.DataFrame({\n",
    "        'MACD': macd,\n",
    "        'MACD_signal': macdsignal,\n",
    "        'RSI': rsi,\n",
    "        'CMO': cmo,\n",
    "        'MOM': mom,\n",
    "        'Upper_BB': upperband,\n",
    "        'Middle_BB': middleband,\n",
    "        'Lower_BB': lowerband,\n",
    "        'SMA_SHORT': sma_s,\n",
    "        'SMA_LONG': sma_l,\n",
    "        'EMA': ema,\n",
    "        'SLOWK': slowk,\n",
    "        'SLOWD': slowd,\n",
    "        'ATR': atr,\n",
    "        'OBV': obv,\n",
    "\n",
    "    })\n",
    "    return indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = add_technical_indicators(stock_data)\n",
    "indicators.head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_with_price = pd.concat([indicators, stock_data[\"Adj Close\"]], axis=1, join='inner')\n",
    "indicators_with_price.head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_with_price = indicators_with_price.dropna()\n",
    "indicators_with_price = indicators_with_price.reset_index(drop=True)\n",
    "indicators_with_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Irrelelvant\n",
    "indicators_with_price['Prev_Adj_Close'] = indicators_with_price['Adj Close'].shift(1)\n",
    "indicators_with_price['Return'] = ((indicators_with_price['Adj Close'] - indicators_with_price['Prev_Adj_Close'])/indicators_with_price['Prev_Adj_Close'])*100\n",
    "indicators_with_price['Signal'] = np.where(indicators_with_price['Return'] > 1, 1,\n",
    "                                           np.where(indicators_with_price['Return'] < -1, 2, 0))\n",
    "indicators_with_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not important\n",
    "indicators_with_price[\"Signal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_with_price.dropna(inplace=True)\n",
    "indicators_with_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_with_price.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicators_with_price = indicators_with_price.drop(columns=['Next_Adj_Close', 'Return'])\n",
    "# indicators_with_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_with_price.drop(columns=['Prev_Adj_Close', \"Signal\"], inplace=True)\n",
    "indicators_with_price.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = indicators_with_price[\"Return\"]\n",
    "y_2 = indicators_with_price[\"SMA_SHORT\"]\n",
    "y_3 = indicators_with_price[\"EMA\"]\n",
    "y_4 = indicators_with_price[\"Upper_BB\"]\n",
    "y_5 = indicators_with_price[\"Middle_BB\"]\n",
    "y_6 = indicators_with_price[\"Lower_BB\"]\n",
    "X = np.array(date)\n",
    "\n",
    "trace = go.Scatter(x=X, y=y, mode=\"lines\", name=\"Adj Close\")\n",
    "trace_2 = go.Scatter(x=X, y=y_2, mode=\"lines\", name=\"SMA\")\n",
    "trace_3 = go.Scatter(x=X, y=y_3, mode=\"lines\", name=\"EMA\")\n",
    "trace_4 = go.Scatter(x=X, y=y_4, mode=\"lines\", name=\"Upper_BB\")\n",
    "trace_5 = go.Scatter(x=X, y=y_5, mode=\"lines\", name=\"Middle_BB\")\n",
    "trace_6 = go.Scatter(x=X, y=y_6, mode=\"lines\", name=\"Lower_BB\")\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Stock Price and Volume',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Adj Close', side='left', rangemode='tozero'),\n",
    "    yaxis2=dict(title='SMA', side='right', overlaying='y', rangemode='tozero'),\n",
    "    yaxis3=dict(title='EMA', side='right', overlaying='y', rangemode='tozero'),\n",
    "    yaxis4=dict(title='Upper_BB', side='right', overlaying='y', rangemode='tozero'),\n",
    "    yaxis5=dict(title='Middle_BB', side='right', overlaying='y', rangemode='tozero'),\n",
    "    yaxis6=dict(title='Lower_BB', side='right', overlaying='y', rangemode='tozero'),\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace, trace_2, trace_3, trace_4, trace_5, trace_6], layout=layout)\n",
    "\n",
    "# Show plot\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class RollingWindowDataset(Dataset):\n",
    "    def __init__(self, X, y, window_size, device=\"gpu\"):\n",
    "        self.X = X.clone().detach().to(torch.float).to(device)\n",
    "        self.y = y.clone().detach().to(torch.float).to(device)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Adjust the length to account for window size\n",
    "        return len(self.X) - self.window_size \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure idx is within the valid range\n",
    "        if idx + self.window_size > len(self.X):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        X_window = self.X[idx : idx + self.window_size]\n",
    "        \n",
    "        y_target = self.y[idx + self.window_size]  \n",
    "\n",
    "        return X_window.clone().detach().to(torch.float), y_target.clone().detach().to(torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = indicators_with_price.iloc[:,:-1]\n",
    "y = indicators_with_price.iloc[:,-2]\n",
    "\n",
    "signal_true = indicators_with_price.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_signal_true = signal_true.iloc[:int(len(X)*0.8)]\n",
    "test_signal_true = signal_true.iloc[int(len(X)*0.8):]\n",
    "test_signal_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Perform hierarchical clustering to find the order of features\n",
    "linked = sch.linkage(sch.distance.pdist(correlation_matrix), method='ward')\n",
    "cluster_order = sch.dendrogram(linked, no_plot=True)['leaves']\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "correlation_matrix_ordered = correlation_matrix.iloc[cluster_order, cluster_order]\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=correlation_matrix_ordered,\n",
    "                    x=correlation_matrix_ordered.columns,\n",
    "                    y=correlation_matrix_ordered.columns,\n",
    "                    colorscale='Viridis',\n",
    "                    text=correlation_matrix_ordered.round(2).values,  \n",
    "                    texttemplate=\"%{text}\",\n",
    "                    textfont={\"size\":9}  \n",
    "                    ))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='Ordered Correlation Matrix',\n",
    "    xaxis_title=\"Variables\",\n",
    "    yaxis_title=\"Variables\",\n",
    "    xaxis=dict(side='bottom'),\n",
    "    yaxis=dict(autorange='reversed'),\n",
    "    width=1000,  \n",
    "    height=1000,  \n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X.iloc[:, cluster_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "y_test.head(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame()\n",
    "X_test_df = pd.DataFrame()\n",
    "scaler_dict = {}\n",
    "\n",
    "X_train_df = X_train\n",
    "X_test_df = X_test\n",
    "\n",
    "for column in X_train.columns:\n",
    "\n",
    "    if column not in [\"Adj Close\", \"Return\"]:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train[[column]].values)\n",
    "        X_train_df[column] = X_train_scaled\n",
    "            \n",
    "        X_test_scaled = scaler.transform(X_test[[column]].values)\n",
    "        X_test_df[column] = X_test_scaled\n",
    "\n",
    "        scaler_dict[column] = scaler\n",
    "\n",
    "\n",
    "X_train_df.head(24)\n",
    "\n",
    "features = X_train_df.columns\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_adj = MinMaxScaler()\n",
    "scaler_adj.fit(X_train[[\"Adj Close\"]].values)\n",
    "\n",
    "X_train_df['Adj Close'] = scaler_adj.transform(X_train[['Adj Close']].values).flatten()\n",
    "X_test_df['Adj Close'] = scaler_adj.transform(X_test[['Adj Close']].values).flatten()\n",
    "\n",
    "y_train = scaler_adj.transform(y_train.values.reshape(-1,1)).flatten()\n",
    "y_test = scaler_adj.transform(y_test.values.reshape(-1,1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_train_df.corr()\n",
    "\n",
    "# Create the heatmap with text\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=correlation_matrix,\n",
    "                    x=correlation_matrix.columns,\n",
    "                    y=correlation_matrix.columns,\n",
    "                    colorscale='Viridis',\n",
    "                    text=correlation_matrix.round(2).values,  # Rounded values for display\n",
    "                    texttemplate=\"%{text}\",\n",
    "                    textfont={\"size\":9}  # Adjust text size if necessary\n",
    "                    ))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix',\n",
    "    xaxis_title=\"Variables\",\n",
    "    yaxis_title=\"Variables\",\n",
    "    xaxis=dict(side='bottom'),\n",
    "    yaxis=dict(autorange='reversed'),\n",
    "    width=1000,  # or any width you desire\n",
    "    height=1000,  # or any height you desire\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_df.to_numpy(), dtype=torch.float, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float, device=device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_df.to_numpy(), dtype=torch.float, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float, device=device)\n",
    "\n",
    "train_data = RollingWindowDataset(X_train_tensor, y_train_tensor, window_size=time_step, device=device)\n",
    "test_data = RollingWindowDataset(X_test_tensor, y_test_tensor, window_size=time_step, device=device)\n",
    "\n",
    "print(test_data.__getitem__(0)[1])\n",
    "print(test_data.__getitem__(1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        # Originally: pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Corrected to keep as [max_len, d_model] for easier indexing\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        # Correctly index into pe to accommodate x's sequence length\n",
    "        # Ensure x and pe have compatible sequence lengths for broadcasting\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Time2Vec, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(1, 1)  # Linear part\n",
    "        self.periodic = nn.Linear(1, d_model - 1)  # Periodic part\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape [batch_size, seq_len, 1] (1 for time dimension)\n",
    "        linear_part = self.linear(x)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Apply periodic transformation\n",
    "        sin_trans = torch.sin(self.periodic(x))\n",
    "        cos_trans = torch.cos(self.periodic(x))\n",
    "        periodic_part = torch.cat((sin_trans, cos_trans), -1)[:, :, :self.d_model-1]\n",
    "        \n",
    "        return torch.cat((linear_part, periodic_part), -1)  # Concatenate linear and periodic parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSeriesTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, d_model, n_head, num_encoder_layers, dropout_prob, output_dim):\n",
    "#         super(TimeSeriesTransformer, self).__init__()\n",
    "#         self.time2vec = Time2Vec(d_model=time2vec_dim)  # Assuming time2vec_dim <= d_model\n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, dropout=dropout_prob, batch_first=True)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "#         self.output_layer = nn.Linear(d_model, output_dim)\n",
    "\n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "#         return mask\n",
    "\n",
    "#     def forward(self, src):\n",
    "#         time_indices = src.size(1)\n",
    "#         # time_indices should be of shape [batch_size, seq_len, 1]\n",
    "#         time_embeddings = self.time2vec(time_indices)  # Generate time embeddings\n",
    "#         src = torch.cat((src, time_embeddings), -1)  # Concatenate src features with time embeddings\n",
    "#         src = self.dropout(src)\n",
    "\n",
    "#         mask = self.generate_square_subsequent_mask(src.size(1)).to(device)\n",
    "\n",
    "#         output = self.transformer_encoder(src, mask=mask)  # Ensure src is [batch_size, seq_len, d_model] due to batch_first=True\n",
    "#         output = self.dropout(output)\n",
    "#         output = self.output_layer(output[:, -1, :])  # Taking the last time step; adjust as needed\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home/arda/Turkcell/images/Screenshot from 2024-02-19 12-26-08.png\" alt=\"Alt text\">\n",
    "<img src=\"/home/arda/Turkcell/images/Screenshot from 2024-02-19 12-26-54.png\" alt=\"Alt text\">\n",
    "<img src=\"/home/arda/Turkcell/images/Screenshot from 2024-02-19 12-27-52.png\" alt=\"Alt text\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_head, num_encoder_layers, dropout_prob, output_dim):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
    "        # self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, dropout=dropout_prob, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_embedding(src).to(device)  # [batch_size, seq_len, input_dim] -> [batch_size, seq_len, d_model]\n",
    "        # src = self.dropout(src)\n",
    "        src = self.pos_encoder(src).to(device)\n",
    "        # src = self.dropout(src)\n",
    "\n",
    "        output = self.transformer_encoder(src)  # Ensure src is [batch_size, seq_len, d_model] due to batch_first=True\n",
    "        # output = self.dropout(output)\n",
    "        output = self.output_layer(output[:, -1, :])  # Taking the last time step; adjust as needed\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelActioner:\n",
    "    \n",
    "    def __init__(self, train_data, test_data, device):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    \n",
    "    def train_validate(self, config, trial):\n",
    "\n",
    "        # batch_size = config[\"batch_size\"]\n",
    "        # epochs = config[\"epochs\"]\n",
    "        # d_model = config[\"d_model\"]\n",
    "        # num_encoder_layers = config[\"num_encoder_layers\"]\n",
    "        # n_head = config[\"n_head\"]\n",
    "        # learning_rate = config[\"learning_rate\"]\n",
    "        # dropout_prob = config[\"dropout_prob\"]\n",
    "        # weight_decay = config[\"weight_decay\"]\n",
    "        # lr_step_size = config[\"lr_step_size\"]\n",
    "        # gamma = config[\"gamma\"]\n",
    "\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        epochs = config[\"epochs\"]\n",
    "        d_model = 512\n",
    "        num_encoder_layers = config[\"num_encoder_layers\"]\n",
    "        n_head = 8\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        dropout_prob = config[\"dropout_prob\"]\n",
    "        weight_decay = config[\"weight_decay\"]\n",
    "        lr_step_size = config[\"lr_step_size\"]\n",
    "        gamma = config[\"gamma\"]\n",
    "\n",
    "\n",
    "        n_splits = 3\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for fold, (train_ids, val_ids) in enumerate(tscv.split(self.train_data)):\n",
    "            print(f'Starting fold {fold+1}:')\n",
    "            \n",
    "            self.model = TimeSeriesTransformer(input_dim=self.train_data.__getitem__(0)[0].shape[1], d_model=d_model, n_head=n_head, num_encoder_layers=num_encoder_layers, dropout_prob=dropout_prob, output_dim=1).to(self.device)\n",
    "\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "            scheduler = ReduceLROnPlateau(self.optimizer, patience=lr_step_size, factor=gamma, mode=\"min\", verbose=True) \n",
    "\n",
    "            train_subset = Subset(self.train_data, train_ids)\n",
    "            val_subset = Subset(self.train_data, val_ids)\n",
    "            \n",
    "            # Creating data loader\n",
    "            train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=False)\n",
    "            val_loader = DataLoader(dataset=val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Training Loop\n",
    "            for epoch in range(epochs):\n",
    "                print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "\n",
    "                running_loss = 0.0\n",
    "                total_sample_train = 0\n",
    "\n",
    "                print(\"hidden none\")\n",
    "\n",
    "                # self.model.hidden_none()\n",
    "                self.model.train()\n",
    "\n",
    "                for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    target = target.view(-1,1) \n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    preds = self.model(data)\n",
    "\n",
    "                    loss = self.criterion(preds, target)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step() # Update model params\n",
    "\n",
    "                    running_loss += loss.item() * data.size(0)\n",
    "                    total_sample_train += data.size(0)\n",
    "\n",
    "                train_loss = running_loss/total_sample_train\n",
    "                #print(f\"train loss: {train_loss}\")\n",
    "                self.model.eval()\n",
    "                val_running_loss = 0.0\n",
    "                total_sample_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                        data, target = data.to(self.device), target.to(self.device)\n",
    "                        target = target.view(-1,1)\n",
    "\n",
    "                        preds = self.model(data)\n",
    "                        loss = self.criterion(preds, target)\n",
    "\n",
    "                        val_running_loss += loss.item() * data.size(0)\n",
    "                        total_sample_val += data.size(0)\n",
    "                \n",
    "                val_loss = val_running_loss/total_sample_val\n",
    "                val_losses.append(val_loss)\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                unique_step = fold * epochs + epoch\n",
    "                trial.report(val_loss, unique_step)\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "                print(f'Current Learning Rate: {current_lr}')\n",
    "                print(f\"train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "                \n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        print(f\"Mean validation loss: {mean_val_loss}\")\n",
    "        return mean_val_loss\n",
    "    \n",
    "                    \n",
    "    def train(self, config):\n",
    "\n",
    "        # batch_size = config[\"batch_size\"]\n",
    "        # epochs = config[\"epochs\"]\n",
    "        # d_model = config[\"d_model\"]\n",
    "        # num_encoder_layers = config[\"num_encoder_layers\"]\n",
    "        # n_head = config[\"n_head\"]\n",
    "        # learning_rate = config[\"learning_rate\"]\n",
    "        # dropout_prob = config[\"dropout_prob\"]\n",
    "        # weight_decay = config[\"weight_decay\"]\n",
    "        # lr_step_size = config[\"lr_step_size\"]\n",
    "        # gamma = config[\"gamma\"]\n",
    "\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        epochs = config[\"epochs\"]\n",
    "        d_model = 512\n",
    "        num_encoder_layers = config[\"num_encoder_layers\"]\n",
    "        n_head = 8\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        dropout_prob = config[\"dropout_prob\"]\n",
    "        weight_decay = config[\"weight_decay\"]\n",
    "        lr_step_size = config[\"lr_step_size\"]\n",
    "        gamma = config[\"gamma\"]\n",
    "\n",
    "        self.model = TimeSeriesTransformer(input_dim=self.train_data.__getitem__(0)[0].shape[1], d_model=d_model, n_head=n_head, num_encoder_layers=num_encoder_layers, dropout_prob=dropout_prob, output_dim=1).to(self.device)\n",
    "\n",
    "        # Update optimizer with updated lr\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Creating data loader\n",
    "        train_loader = DataLoader(dataset=self.train_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, patience=lr_step_size, factor=gamma, mode=\"min\", verbose=True)  \n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            total_sample_train = 0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                target = target.view(-1,1)  \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(data)\n",
    "\n",
    "                loss = self.criterion(preds, target)\n",
    "                #loss = loss.mean()\n",
    "                loss.backward()\n",
    "                self.optimizer.step() # Update model params\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                total_sample_train += data.size(0)\n",
    "\n",
    "            train_loss = running_loss/total_sample_train\n",
    "            #print(f\"train loss: {train_loss}\")\n",
    "            scheduler.step(train_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print(f'Current Learning Rate: {current_lr}')\n",
    "            print(f\"train_loss: {train_loss}\")\n",
    "        \n",
    "        return self.model\n",
    "            \n",
    "    \n",
    "    def test(self, config):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        all_preds = []\n",
    "\n",
    "        test_loader = DataLoader(dataset=self.test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        running_loss = .0\n",
    "        total_sample = 0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                target = target.view(-1,1)\n",
    "                \n",
    "                preds = self.model(data)\n",
    "                loss = self.criterion(preds, target)\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                total_sample += data.size(0)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            test_loss = running_loss/total_sample\n",
    "            print(f\"test_loss: {test_loss}\")\n",
    "\n",
    "        return all_preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    config = {\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 128, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 10, 100),\n",
    "        # \"d_model\": trial.suggest_int(\"d_model\", 10, 150),\n",
    "        \"num_encoder_layers\": trial.suggest_int(\"num_encoder_layers\", 1, 8),\n",
    "        # \"n_head\": trial.suggest_int(\"n_head\", 3, 6),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1),\n",
    "        \"dropout_prob\": trial.suggest_float(\"dropout_prob\", 0.0, 0.3),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True),\n",
    "        \"lr_step_size\": trial.suggest_int(\"lr_step_size\", 10, 100), \n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
    "    }\n",
    "\n",
    "    trainer = ModelActioner(train_data, test_data, device)\n",
    "\n",
    "    val_loss = trainer.train_validate(config, trial)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"Transformer-Tunner\"\n",
    "storage_url = \"sqlite:///db.sqlite3\"\n",
    "\n",
    "storage = optuna.storages.RDBStorage(url=storage_url)\n",
    "\n",
    "# Check if the study exists\n",
    "study_names = [study.study_name for study in optuna.study.get_all_study_summaries(storage=storage)]\n",
    "if study_name in study_names:\n",
    "    # Delete the study if it exists\n",
    "    print(f\"Deleting study '{study_name}'\")\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_url)\n",
    "else:\n",
    "    print(f\"Study '{study_name}' does not exist in the storage.\")\n",
    "    \n",
    "\n",
    "study = optuna.create_study(direction='minimize', \n",
    "                            storage=storage_url, \n",
    "                            sampler=TPESampler(),\n",
    "                            pruner=optuna.pruners.HyperbandPruner(\n",
    "                            min_resource=30,  \n",
    "                            max_resource=100,  \n",
    "                            reduction_factor=3,  \n",
    "                            ),\n",
    "                            study_name=study_name,\n",
    "                            load_if_exists=False)\n",
    "\n",
    "pbar = tqdm(total=15, desc='Optimizing', unit='trial')\n",
    "\n",
    "def callback(study, trial):\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix_str(f\"Best Value: {study.best_value:.4f}\")\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=15, callbacks=[callback])\n",
    "pbar.close()\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelActioner(train_data, test_data, device)\n",
    "model = trainer.train(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.test(trial.params)\n",
    "preds = np.array(preds)\n",
    "\n",
    "y_true = y_test[time_step:]\n",
    "y_true = scaler_adj.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
    "preds = scaler_adj.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_true, preds)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "rmse = mean_squared_error(y_true, preds, squared=False)\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "\n",
    "r2 = r2_score(y_true, preds)\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_true, preds)*100\n",
    "print(f'mape Score: % {mape:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = stock_data[\"Adj Close\"].iloc[test_index:]\n",
    "stock_price=stock_price.reset_index()\n",
    "stock_price=stock_price.drop(columns=[\"index\"])\n",
    "stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = pd.DataFrame(preds, columns=['pred'])\n",
    "signals[\"next_day\"] = pd.DataFrame(y_true)\n",
    "signals[\"today\"] = stock_price\n",
    "signals[\"Signal_Pred\"] = (signals[\"today\"] < signals[\"pred\"]).astype(int)\n",
    "signals[\"Signal_True\"] = (signals[\"today\"] < signals[\"next_day\"]).astype(int)\n",
    "\n",
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals[\"Signal_Pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals[\"Date\"] = date_test\n",
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = stock_data[\"Adj Close\"].iloc[test_index:]\n",
    "stock_price=stock_price.reset_index()\n",
    "stock_price=stock_price.drop(columns=[\"index\"])\n",
    "stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_test[\"Date\"] = date_test[\"Date\"].dt.strftime('%Y-%m-%d')\n",
    "date_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.array(date_test).flatten(), y=stock_data[\"Adj Close\"].iloc[test_index:], mode='lines', name='TSLA Stock Price'))\n",
    "\n",
    "# Buy and sell signals\n",
    "buy_signals = signals[signals['Signal_Pred'] == 1]\n",
    "sell_signals = signals[signals['Signal_Pred'] == 0]\n",
    "\n",
    "# Ensure that buy and sell prices are aligned with the signals by matching on the 'Date' column\n",
    "buy_prices = stock_data[stock_data['Date'].isin(buy_signals['Date'])][\"Adj Close\"]\n",
    "sell_prices = stock_data[stock_data['Date'].isin(sell_signals['Date'])][\"Adj Close\"]\n",
    "\n",
    "\n",
    "# Plot buy signals\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=buy_signals['Date'], \n",
    "    y=buy_prices, \n",
    "    mode='markers', \n",
    "    name='Buy', \n",
    "    marker=dict(color='green', size=10, symbol='triangle-up')\n",
    "))\n",
    "\n",
    "\n",
    "# Plot sell signals\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sell_signals['Date'], \n",
    "    y=sell_prices, \n",
    "    mode='markers', \n",
    "    name='Sell', \n",
    "    marker=dict(color='red', size=10, symbol='triangle-down')\n",
    "))\n",
    "\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Stock Price with Buy and Sell Signals',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price',\n",
    "    xaxis_rangeslider_visible=False,\n",
    "    height = 700,\n",
    "    width=1280\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_pred = go.Scatter(\n",
    "    x=np.array(date_test).flatten(),\n",
    "    y=signals['pred'],\n",
    "    mode='lines+markers',\n",
    "    name='Predicted'\n",
    ")\n",
    "\n",
    "trace_true = go.Scatter(\n",
    "    x=np.array(date_test).flatten(),\n",
    "    y=signals['next_day'],\n",
    "    mode='lines+markers',\n",
    "    name='Actual Next Day'\n",
    ")\n",
    "\n",
    "# Define the layout\n",
    "layout = go.Layout(\n",
    "    title='Predicted vs Actual Next Day Values',\n",
    "    xaxis=dict(title='Index'),\n",
    "    yaxis=dict(title='Value'),\n",
    "    height=700\n",
    ")\n",
    "\n",
    "# Create the figure and add traces\n",
    "fig = go.Figure(data=[trace_pred, trace_true], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
